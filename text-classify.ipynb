{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification: the basics\n",
    "\n",
    "The aim of this notebook is to take what I learnt during the DSTL Multiple Label Classifcation data science challenge and distill it into prototype code so that I have a template for any text classification I do in future.\n",
    "\n",
    "This competition did not allow you to retain the data so everything here is done using data provided by sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting some data\n",
    "\n",
    "Download some example data which has three catagories which appear evenly. Then store the text itself and the catagory in a pandas dataframe with two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Another fish to check out is Richard Rast -- h...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: As the subject says - Can I use a 4052 for d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am looking for current sources for lists of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nBut why do you characterize this as a \"fli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nIt was more than a theoretical concept; it w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\n\\nThe name is rather descriptive.  It's a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My mom has just been diagnosed with cystic bre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nThe yearly chest x-ray provides a minute a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I've recently listened to a tape by Dr. Stanis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We've just been donated a large machine for us...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  topic\n",
       "0   Another fish to check out is Richard Rast -- h...      2\n",
       "1   : As the subject says - Can I use a 4052 for d...      0\n",
       "2   I am looking for current sources for lists of ...      1\n",
       "3   \\n\\nBut why do you characterize this as a \"fli...      1\n",
       "4   \\nIt was more than a theoretical concept; it w...      2\n",
       "5   \\n\\n\\nThe name is rather descriptive.  It's a ...      2\n",
       "6   My mom has just been diagnosed with cystic bre...      1\n",
       "7   \\n\\nThe yearly chest x-ray provides a minute a...      1\n",
       "9   I've recently listened to a tape by Dr. Stanis...      1\n",
       "10  We've just been donated a large machine for us...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catagories = ['sci.med', 'sci.electronics', 'sci.space']\n",
    "text_data = fetch_20newsgroups(categories=catagories,\n",
    "                               random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['text'] = text_data.data\n",
    "df['topic'] = text_data.target\n",
    "\n",
    "# Remove blank lines\n",
    "df = df[ df['text']!= \"\" ]\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These catagories are lablled 0, 1, and 2. By resetting all the labels of catagory 2 to 0 we can simulate unbalanced classes on a simply 0/1 classification problem. Let's count the number of documents in each catagory before and after this transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0    577\n",
       "1    583\n",
       "2    580"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_catagories = pd.DataFrame()\n",
    "df_catagories['count'] = df.groupby('topic').count()\n",
    "df_catagories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0   1157\n",
       "1    583"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['topic']==2, 'topic' ] = 0\n",
    "\n",
    "df_catagories = pd.DataFrame()\n",
    "df_catagories['count'] = df.groupby('topic').count()\n",
    "df_catagories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And finally split the data into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preparing the text\n",
    "\n",
    "Next we need to define a function which tokenise the text (in this case converting it from a string to a list of words), stems the words (so converts 'run', 'runs', and 'running' all to 'run') and generally cleans up the text by removing punctuation, putting all the text into lower case etc. Stemming and these cleaning steps may or may not improve the classifier. It is always worth turning these off and on to see whether they do improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tokeniser(text):\n",
    "    \n",
    "    # Remove any whitespace at the start and end of the string\n",
    "    # and remove any stray tabs and newline characters\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove any weird unicode characters\n",
    "    if isinstance(text, unicode):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore')\n",
    "        \n",
    "    # Convert hyphens and slashes to spaces\n",
    "    text = re.sub(r'[-/]+',' ',text)\n",
    "    \n",
    "    # Remove remaining punctuation\n",
    "    text = text.translate(None, string.punctuation)\n",
    "    \n",
    "    # Convert the text to lowercase and use nltk tokeniser\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Define a list of stopwords apart from the word 'not'\n",
    "    stops = set(stopwords.words('english')) - set(('not'))\n",
    "\n",
    "    # Define stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    return [stemmer.stem(i) for i in tokens if i not in stops]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us try out this function on a test string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'text', u'albeit', u'exampl', u'test', u'text', u'demonstr', u'arent']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser(\"HERE is some text, albeit example/test text, THAT demonstrates what we are and aren't doing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Fitting the model\n",
    "\n",
    "When we create the model itself we are going to use weighting to correct for class imbalance. To do this we first need to know the number of 1s and 0s in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1s: 418\n",
      "Number of 0s: 800\n"
     ]
    }
   ],
   "source": [
    "num_1s = df_train[df_train['topic']==1]['topic'].count()\n",
    "num_0s = df_train[df_train['topic']==0]['topic'].count()\n",
    "\n",
    "print \"Number of 1s:\", num_1s\n",
    "print \"Number of 0s:\", num_0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we define and fit a sklearn pipeline. This pipeline chains together a hashing vectorizer, a term frequency-inverse document frequency (TF-IDF) transform, singular value decomposition (SVD), and then classification using XGBoost.\n",
    "\n",
    "The hashing vectoriser represents each unique word (after preprocessing which could include stemming and removing punctuation and stopwords) by a number and counts the number of times each word occurs in every document. Each document could then be represented by a vector where each element represents corresponds to a different word and the value of the element is equal to the number of times that word occurred in the document. A hashing vectoriser uses less memory then a count vectoriser but the downside is that you cannot go from the numerical representation of the words back to the words themselves, this makes interupting intermediate steps more difficult.\n",
    "\n",
    "The TF-IDF transform weights each of the components of these vectors by a number that takes into account how common the word is across the whole corpus and how common it is in that particular document. This means that words that occur very frequently are down weighted and rare words are upweighted.\n",
    "\n",
    "Truncated SVD is a form of principal component analysis which works well with sparce matrices. This reduces the number of dimensions by breaking down the matrix that descirbes the correlations between words across the documents into eigenvalues/eigenvevectors. Only a subset of eigenvectors are retained, this subset being the eigenvecots that capture the most variance. This approach corresponds to Latent Semantic Analysis.\n",
    "\n",
    "After this the XGBoost version of a gradient boosted decision tree classifier is fitted to the transformed, dimensionally reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', HashingVectorizer(analyzer=u'word', binary=False, decode_error='replace',\n",
       "         dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "         lowercase=True, n_features=1048576, ngram_range=(1, 3),\n",
       "         non_negative=False, norm=u'l2', preprocessor=None,\n",
       "         ...eg_lambda=1,\n",
       "       scale_pos_weight=1.9138755980861244, seed=42, silent=True,\n",
       "       subsample=1))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the pipeline\n",
    "vectoriser = Pipeline([\n",
    "    ('vect', HashingVectorizer(tokenizer=tokeniser,\n",
    "                               decode_error='replace',\n",
    "                               #strip_accents='unicode',\n",
    "                               ngram_range=(1,3))\n",
    "    ),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svd', TruncatedSVD(n_components=100,\n",
    "                         random_state=42)\n",
    "    ),\n",
    "    ('xgb', XGBClassifier(max_depth=6,\n",
    "                          seed=42,\n",
    "                          n_estimators=200,\n",
    "                          scale_pos_weight=num_0s/num_1s)\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Fit the model \n",
    "vectoriser.fit(df_train['text'], df_train['topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model (which was fitted on the training data) can then be used to produce predictions for the topics contained within the test data. These predictions will later be compared to the actual, known values in order to determine the accuracy of the model.\n",
    "\n",
    "Note, pandas can not keep track of all the copies of a dataframe so throws up an error with the line below which stores the predictions back into the test dataframe. However, this line does work so I have disabled warnings beforehand and re-activated them afterwards in order to avoid the warning message. This is not strictly necessary but makes the ouput prettier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>I am in the midst of designing a project which...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>\\nWhatabout, Schools, Universities, Rich Indiv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>Umm, perhaps you could explain what 'rights' ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>\\nReading this definition, I wonder: when shou...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>\\n\\n\\nIf you want to have some fun.\\n\\nPlug th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>Actually, they are legal! I not familiar with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>Subject: options before back surgery for protr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>I have a HP 1740 scope that (I think) has a pr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>\\nSure.  Contact the World Space Foundation.  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  topic  predict\n",
       "488   I am in the midst of designing a project which...      0        0\n",
       "1539                                           \\n\\n\\n\\n      0        1\n",
       "969   \\nWhatabout, Schools, Universities, Rich Indiv...      0        0\n",
       "1025   Umm, perhaps you could explain what 'rights' ...      0        0\n",
       "719   \\nReading this definition, I wonder: when shou...      1        0\n",
       "277   \\n\\n\\nIf you want to have some fun.\\n\\nPlug th...      0        0\n",
       "443   Actually, they are legal! I not familiar with ...      0        0\n",
       "1336  Subject: options before back surgery for protr...      1        1\n",
       "1160  I have a HP 1740 scope that (I think) has a pr...      0        0\n",
       "624   \\nSure.  Contact the World Space Foundation.  ...      0        0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppress pandas warning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Make predictions for the test data and store \n",
    "# the results back into the test dataframe\n",
    "df_test['predict'] = vectoriser.predict(df_test['text'])\n",
    "\n",
    "# Reset pandas warning\n",
    "pd.options.mode.chained_assignment = 'warn'\n",
    "\n",
    "# Show the first ten entries in the dataframe \n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluate the model\n",
    "\n",
    "The precision, recall and f1-score can be outputted to see how well the model is performing on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.96      0.94       357\n",
      "          1       0.91      0.81      0.86       165\n",
      "\n",
      "avg / total       0.92      0.92      0.91       522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(df_test['topic'], df_test['predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "You can use joblib to save fitted sklearn models and then reload them later. For example, you might want to save a model and later use it to make more predictions, or if the model has a time consuming preprocessing step that you do not want to repeat whilst hyperparameter tuining these could be split out into a seperate model which is run and saved. For example, the tfidf and svd steps here could be split out into a seperate preprocessing pipeline and then could be fitted once before tuning the hyperparameters of the classifier itself.\n",
    "\n",
    "Below are the joblib commands (commented out). Joblib is recommended over pickle for sklearn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To save a model use dump\n",
    "# joblib.dump(vectoriser, \"./saved_model.pkl\")\n",
    "\n",
    "# To load a saved model use load\n",
    "# vectoriser = joblib.load(\"./saved_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "One method of choosing hyperparameters is to save the values you want to try in a dictionary and then use GridSearchCV to perform cross-validation to estimate the training error for all combinations, choosing the parameters that gives the best results according to the metric chosen. These best parameters can be outputted and the model outputted from the grid search can be used for prediction like regular sklearn models.\n",
    "\n",
    "Here, the name given to the xgboost classifier ('xgb') has to be given so that GridSearchCV knowns which part of the pipeline these parameters are associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb__n_estimators': 100, 'xgb__max_depth': 7}\n"
     ]
    }
   ],
   "source": [
    "params = dict(xgb__max_depth=[3,7], xgb__n_estimators=[100])\n",
    "\n",
    "grid = GridSearchCV(vectoriser, param_grid=params, scoring='f1_micro', cv=2)\n",
    "\n",
    "grid.fit(df_train['text'], df_train['topic'])\n",
    "\n",
    "print grid.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
