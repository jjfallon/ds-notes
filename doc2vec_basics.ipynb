{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec with gensim\n",
    "\n",
    "This notebook demonstrates the basic commands needed to run doc2vec using the functions provided by by working through a toy example. This is using python 2.7 and gensim version 2.2.0.\n",
    "\n",
    "Doc2vec is an extension of word2vec which converts documents (which could be any sequences of words, be they paragraphs, comments, or even whole documents). This notebook only demonstrates how to use the technique, it does not explain how it works. For details consult the paper by Le and Mikolov: https://arxiv.org/abs/1405.4053."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Documents to vectors\n",
    "In this first section we will show how to produce a vector for each document. First load the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define a couple of helper functions. One which splits ('tokenises') a string into a list of words, and one which takes puts our data into the data structure gensim is expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenise on whitespace\n",
    "def tokeniser(text):\n",
    "    return re.findall(r\"\\S+\", text)\n",
    "\n",
    "# Create labelled sentence data structure\n",
    "def labeller_doc(tag, text):\n",
    "    return TaggedDocument(tokeniser(text), [tag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the tokeniser does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'example', 'string', 'of', 'text']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser('an example string of text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple tokeniser, much more advanced ones are available. \n",
    "\n",
    "Doc2vec expects data in the 'TaggedDocument' format, which is a list of words within the documents and a list of IDs. In this case we are creating one vector per document so each document needs one, unique ID. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['an', 'example', 'document'], tags=['doc1'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_document = 'an example document'\n",
    "ex_id = 'doc1'\n",
    "labeller_doc(ex_id, ex_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the example where we have text within a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example data\n",
    "df = pd.DataFrame()\n",
    "df['text'] = [\n",
    "                'this is text', \n",
    "                'any more text?', \n",
    "                'even more text!', \n",
    "                'words, all words'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the dataframe IDs as the document IDs and convert all the documents into the required TaggedDocument format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['this', 'is', 'text'], tags=['0'])\n",
      " TaggedDocument(words=['any', 'more', 'text?'], tags=['1'])\n",
      " TaggedDocument(words=['even', 'more', 'text!'], tags=['2'])\n",
      " TaggedDocument(words=['words,', 'all', 'words'], tags=['3'])]\n"
     ]
    }
   ],
   "source": [
    "# Get data into correct format\n",
    "documents =  df.apply(lambda x: labeller_doc(str(x.name), x['text']), axis=1).values\n",
    "\n",
    "print documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can build the doc2vec model and delete some of the items created to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Doc2Vec(documents = documents,\n",
    "        size = 2,\n",
    "        seed = 42,\n",
    "        min_count = 1,\n",
    "        max_vocab_size = None,\n",
    "        window = 5,\n",
    "        iter = 5)\n",
    "\n",
    "# Free up memory\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve vectors for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03483918, -0.16661185],\n",
       "       [ 0.18773492,  0.22185083],\n",
       "       [-0.12609929,  0.06258722],\n",
       "       [ 0.11312576,  0.16608955]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.doctag_syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the IDs associated with each vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0', '3', '2']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.doctags.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this shows that the order of the vectors does not follow that of the original data frame. But you can retrieve the text associated with each of these vectors in the correct order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>any more text?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>words, all words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>even more text!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               text\n",
       "1    any more text?\n",
       "0      this is text\n",
       "3  words, all words\n",
       "2   even more text!"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.DataFrame()\n",
    "df_new['text'] = df.loc[map(int, model.docvecs.doctags.keys()), 'text']\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors for new documents can be inferred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24704833,  0.23743619], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence = ['one', 'more', 'sentence']\n",
    "model.infer_vector(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can the vectors for multiple sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.07819276, -0.06925946],\n",
       "        [ 0.01494676, -0.2055566 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentences = [ ['first', 'text'], ['second', 'test']]\n",
    "np.matrix(map(model.infer_vector, new_sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
