{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec with gensim\n",
    "\n",
    "This notebook demonstrates the basic commands needed to train a doc2vec model using the functions provided by gensim by working through a toy example. Here python 2.7 and gensim version 2.2.0 are used.\n",
    "\n",
    "doc2vec is an extension of word2vec which converts documents (these could be any sequences of words â€“ paragraphs, comments, or even whole documents). This notebook only demonstrates how to use the technique; it does not explain how it works. For details consult the paper by Le and Mikolov: https://arxiv.org/abs/1405.4053."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Documents to vectors\n",
    "In this first section we will show how to produce a vector for each document. First load the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions\n",
    "Next we need to define a couple of helper functions. One which splits ('tokenises') a string into a list of words, and one which takes puts our data into the data structure gensim is expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenise on whitespace\n",
    "def tokeniser(text):\n",
    "    return re.findall(r\"\\S+\", text)\n",
    "\n",
    "# Create labelled sentence data structure\n",
    "def labeller_id(ID, text):\n",
    "    return TaggedDocument(tokeniser(text), [ID])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the tokeniser does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'example', 'string', 'of', 'text']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser('an example string of text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple tokeniser, much more advanced ones are available. \n",
    "\n",
    "Doc2vec expects data in the 'TaggedDocument' format, which is a list of words within the documents and a list of IDs. In this case we are creating one vector per document so each document needs one, unique ID. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['an', 'example', 'document'], tags=['doc1'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_document = 'an example document'\n",
    "ex_id = 'doc1'\n",
    "labeller_id(ex_id, ex_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data into the right format\n",
    "Let's consider the example where we have text within a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create example data\n",
    "df = pd.DataFrame()\n",
    "df['text'] = [\n",
    "                'this is text', \n",
    "                'any more text?', \n",
    "                'even more text!', \n",
    "                'words, all words'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the dataframe IDs as the document IDs and convert all the documents into the required TaggedDocument format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([TaggedDocument(words=['this', 'is', 'text'], tags=['0']),\n",
       "       TaggedDocument(words=['any', 'more', 'text?'], tags=['1']),\n",
       "       TaggedDocument(words=['even', 'more', 'text!'], tags=['2']),\n",
       "       TaggedDocument(words=['words,', 'all', 'words'], tags=['3'])], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data into correct format\n",
    "documents =  df.apply(lambda x: labeller_id(str(x.name), x['text']), axis=1).values\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "Now we can build the doc2vec model and then delete some of the items created by gensim during the model build in order to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Doc2Vec(documents = documents,\n",
    "        size = 2,\n",
    "        seed = 42,\n",
    "        min_count = 1,\n",
    "        max_vocab_size = None,\n",
    "        window = 5,\n",
    "        iter = 5)\n",
    "\n",
    "# Free up memory\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the vectors produced for each of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03483918, -0.16661185],\n",
       "       [ 0.18773492,  0.22185083],\n",
       "       [-0.12609929,  0.06258722],\n",
       "       [ 0.11312576,  0.16608955]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.doctag_syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the IDs associated with each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0', '3', '2']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.doctags.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this shows that the order of the vectors does not follow that of the original dataframe. But you can retrieve the text associated with each of these vectors in the correct order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      any more text?\n",
       "0        this is text\n",
       "3    words, all words\n",
       "2     even more text!\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[map(int, model.docvecs.doctags.keys()), 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model\n",
    "Vectors for new documents can be inferred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24704833,  0.23743619], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence = ['one', 'more', 'sentence']\n",
    "model.infer_vector(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can the vectors for multiple sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.07819276, -0.06925946],\n",
       "        [ 0.01494676, -0.2055566 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentences = [ ['first', 'text'], ['second', 'test']]\n",
    "np.matrix(map(model.infer_vector, new_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics to vectors\n",
    "\n",
    "Another way in which doc2vec can be used is to learn vectors for topics rather than for individual documents. To do this instead of giving the model a unique ID for each document you instead pass a list of topics. Each document can have multiple topics.\n",
    "\n",
    "Let's demonstrate. First we need to tweak our labeller function so that it takes a list as input rather than a single ID. This reuses the tokeniser function defined previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create labelled sentence data structure when given topic list\n",
    "def labeller_topics(topic_list, text):\n",
    "    return TaggedDocument(tokeniser(text), topic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next define some example data where there are two topics: 'question' and 'cats'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is there any more text?</td>\n",
       "      <td>[question]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>where are the words of the documents</td>\n",
       "      <td>[question]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cats like to chase mice</td>\n",
       "      <td>[cats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cats do not like dogs</td>\n",
       "      <td>[cats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do you like cats?</td>\n",
       "      <td>[question, cats]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text             topic\n",
       "0               is there any more text?        [question]\n",
       "1  where are the words of the documents        [question]\n",
       "2               cats like to chase mice            [cats]\n",
       "3                 cats do not like dogs            [cats]\n",
       "4                     do you like cats?  [question, cats]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create example data\n",
    "df2 = pd.DataFrame()\n",
    "df2['text'] = [\n",
    "    'is there any more text?',\n",
    "    'where are the words of the documents',\n",
    "    'cats like to chase mice',\n",
    "    'cats do not like dogs',\n",
    "    'do you like cats?'\n",
    "    ]\n",
    "\n",
    "# Each document can have multiple topics associated with it (list of topics per document)\n",
    "df2['topic'] = [\n",
    "    ['question'],\n",
    "    ['question'],\n",
    "    ['cats'],\n",
    "    ['cats'],\n",
    "    ['question', 'cats']\n",
    "    ]\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be converted into TaggedDocument form using the new labeller_topics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents2 = df2.apply(lambda x: labeller_topics(x['topic'], x['text']), axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a doc2vec model can be build as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Doc2Vec(documents = documents2,\n",
    "        size = 2,\n",
    "        seed = 42,\n",
    "        min_count = 1,\n",
    "        max_vocab_size = None,\n",
    "        window = 5,\n",
    "        iter = 5)\n",
    "\n",
    "# Free up memory\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the document vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03462205, -0.05810256],\n",
       "       [ 0.10018398,  0.24293993]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.doctag_syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and note only two vectors have been produced. This is encouraging as we only had two topics. Checking the tags of these vectors gives the two topics we were expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats', 'question']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.doctags.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
